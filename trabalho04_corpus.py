# -*- coding: utf-8 -*-
"""Trabalho04_Corpus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YljKr_Zt2lx02GNwG84-B14uCGh0MnY4

Aluna: Vitória Izabel Mendes Pinto

Sua  tarefa  será  transformar  um  conjunto  de  5  sites,  sobre  o  tema  de  processamento  de 
linguagem natural em um conjunto de cinco listas distintas de sentenças. Ou seja, você fará uma função 
que, usando a biblioteca Beautifull Soap, faça a requisição de uma url, e extrai todas as sentenças desta 
url. Duas condições são importantes:  
a) A página web (url) deve apontar para uma página web em inglês contendo, não menos que 
1000 palavras.  
b) O texto desta página deverá ser transformado em um array de senteças.  
 
Para separar as sentenças você pode usar os sinais de pontuação ou as funções da biblibioteca 
Spacy.
"""

import requests
from bs4 import BeautifulSoup

"""# Site 01

https://www.tableau.com/learn/articles/natural-language-processing-examples
"""

site01 = "https://www.tableau.com/learn/articles/natural-language-processing-examples"
html = requests.get(site01).text
bs4_site01 = BeautifulSoup(html, "html.parser")
 
text_array1 = []
get = bs4_site01.find_all('p')
text_dir1 = list(get)

for text in text_dir1:
    text_array1.append(text.get_text())

print(text_array1)

"""# Site 02

https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1
"""

site02 = "https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1"
html = requests.get(site02).text
bs4_site02 = BeautifulSoup(html, "html.parser")
 
text_array2 = []
get = bs4_site02.find_all('p')
text_dir2 = list(get)

for text in text_dir2:
    text_array2.append(text.get_text())

print(text_array2)

"""# Site 03
https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP

"""

site03 = "https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP"
html = requests.get(site03).text
bs4_site03 = BeautifulSoup(html, "html.parser")
 
text_array3 = []
get = bs4_site03.find_all('p')
text_dir3 = list(get)

for text in text_dir3:
    text_array3.append(text.get_text())

print(text_array3)

"""# Site 04
https://hbr.org/2022/04/the-power-of-natural-language-processing

"""

site04 = "https://hbr.org/2022/04/the-power-of-natural-language-processing/"
html = requests.get(site04).text
bs4_site04 = BeautifulSoup(html, "html.parser")
 
text_array4 = []
get = bs4_site04.find_all('p')
text_dir4 = list(get)

for text in text_dir4:
    text_array4.append(text.get_text())

print(text_array4)

"""# Site 05
https://www.thoughtworks.com/en-br/insights/decoder/n/natural-language-processing
"""

site05 = "https://www.thoughtworks.com/en-br/insights/decoder/n/natural-language-processing"
html = requests.get(site05).text
bs4_site05 = BeautifulSoup(html, "html.parser")
 
text_array5 = []
get = bs4_site05.find_all('p')
text_dir5 = list(get)

for text in text_dir5:
    text_array5.append(text.get_text())

print(text_array5)